{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae1ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7adac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9284ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248b0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3528580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b913148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64876d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 624.97 | loss  8.09 | ppl  3272.28\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 579.73 | loss  6.84 | ppl   930.39\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 674.33 | loss  6.41 | ppl   610.67\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 614.32 | loss  6.29 | ppl   536.88\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 603.49 | loss  6.17 | ppl   479.83\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 628.84 | loss  6.15 | ppl   467.22\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 672.15 | loss  6.11 | ppl   450.45\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 627.93 | loss  6.10 | ppl   447.43\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 625.59 | loss  6.02 | ppl   412.56\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 650.74 | loss  6.01 | ppl   408.06\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 641.07 | loss  5.88 | ppl   359.35\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 611.63 | loss  5.96 | ppl   389.28\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 621.37 | loss  5.94 | ppl   381.78\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 691.94 | loss  5.88 | ppl   356.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1958.96s | valid loss  5.78 | valid ppl   325.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 686.25 | loss  5.85 | ppl   346.75\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 652.64 | loss  5.84 | ppl   342.68\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 670.40 | loss  5.66 | ppl   286.00\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 711.72 | loss  5.70 | ppl   297.86\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 645.09 | loss  5.64 | ppl   280.72\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 620.48 | loss  5.67 | ppl   289.65\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 626.52 | loss  5.68 | ppl   292.48\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 721.96 | loss  5.70 | ppl   298.78\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 630.68 | loss  5.64 | ppl   282.69\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 629.77 | loss  5.66 | ppl   287.67\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 629.90 | loss  5.54 | ppl   253.86\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 618.92 | loss  5.63 | ppl   278.98\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 621.09 | loss  5.63 | ppl   278.79\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 627.72 | loss  5.57 | ppl   263.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1972.57s | valid loss  5.65 | valid ppl   283.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 681.94 | loss  5.58 | ppl   266.40\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 628.59 | loss  5.59 | ppl   268.38\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 627.52 | loss  5.40 | ppl   222.18\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 685.65 | loss  5.47 | ppl   236.92\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 783.73 | loss  5.42 | ppl   226.45\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 745.51 | loss  5.45 | ppl   233.84\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 721.70 | loss  5.47 | ppl   238.61\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 653.73 | loss  5.50 | ppl   245.19\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 645.82 | loss  5.45 | ppl   231.90\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 709.31 | loss  5.47 | ppl   236.64\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 749.19 | loss  5.35 | ppl   210.52\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 933.23 | loss  5.45 | ppl   232.28\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 676.75 | loss  5.48 | ppl   240.72\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 720.12 | loss  5.38 | ppl   217.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 2157.02s | valid loss  5.63 | valid ppl   277.95\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3451a886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.54 | test ppl   254.48\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71396d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
